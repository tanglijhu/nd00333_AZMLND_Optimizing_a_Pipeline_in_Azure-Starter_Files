# Optimizing an ML Pipeline in Azure


## Overview

This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.
-----------------------------------------------------------------------------------------------------------------------------------------------------------

## Summary

**In 1-2 sentences, explain the problem statement:** 

This dataset contains general population information including age, job, marital status, education, housing, loan, date, poutcome, etc. We seek to predict whether we are able to target a potential customer based on these information.   

**In 1-2 sentences, explain the solution:** 

The best performing model was Logistic Regression with the following metrics:

- Regularization Strength: 10
- Max Iterations: 50
- Accuracy: 0.91015

The hyperparameters of this best model were:

- C = 10 
- max_iter = 50

-----------------------------------------------------------------------------------------------------------------------------------------------------------

## Scikit-learn Pipeline

**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The pipeline architecture includes: 

- The dataset used for training was obtained from Azure dataset using a provided url: 
datset = Dataset.Tabular.from_delimited_files(path=url)

- The dataset was then transformed to dataframe using pandas, cleaned by deleting any missing or irrelevant data, and split into training and testing data. 

- Hyperparameter tuning was performed by using HyperDriveConfig with specified training transcript and parameters. 

- The classification algorithm used in this study was Logistic regression from sklean. 


**What are the benefits of the parameter sampler you chose?**

The parameter sampler used here was RandomParameterSampling. It supports both of descrete and continuous hyperparameters and also supports early termination of low-performance runs. 


**What are the benefits of the early stopping policy you chose?**

The early stopping policy chosed here was BanditPolicy. It is based on slack factor and evaluation interval. Bandit terminates runs where the primary metric is not within the specified slack factor compared to the best performing run, therefore helping save the total model training time. It is mainly used for aggressive savings with relatively large truncation percentage. 
-----------------------------------------------------------------------------------------------------------------------------------------------------------

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
 
With the metric "Spearman correlation", the best regression model generated by AutoML was Random Forest Regression (StandardScalerWrapper).  

The hyperparameters of this best model were:
"""
min_samples_leaf = 1
min_samples_split = 2
n_estimators = 100
"""

Based on the best model explanations, the feature importance was as "duration > nr.employed > euribor3m > cons.conf.idx > emp.var.rate > pdays > age > cons.price.idx". 

However, if the matric used was "Normalized root mean squared error", the best regression model generated by AutoML was VotingEnsemble, with the feature importance as "duration > nr.employed > cons.conf.idx > euribor3m > pdays > emp.var.rate> age > cons.price.idx". 
-----------------------------------------------------------------------------------------------------------------------------------------------------------

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Performance Comparision: 

- For AutoML with "Spearman correlation" as metric:
spearman correlation: 0.54580
root mean squared error: 0.24351
explained variance: 0.40385
mean absolute error: 0.11225
R2: 0.40364

- For AutoML with "Normalized root mean squared metric":
spearman correlation: 0.48811
root mean squared error: 0.23757
explained variance: 0.43267
mean absolute error: 0.11473
R2: 0.43252

- For SDK Hyperdive hyperparameter tuning with "accuracy" as metric: 
Accuracy: 0.910153

As there is no "accuracy" as the metric in AutoML configuration, there two appoaches can not be compared directly in terms of "accuracy" in this study. However, as R2 values were relatively poor from AutoML training ~ 0.40-0.43 while accuracy from SDK Hyperdive hyperparameter tuning was relatively high ~ 0.91, I would like to say SDK Hyperdive hyperparameter performed better than AutoML. 


Architecture Comparision:

- AutoML does not need any input parameters but generally two: task and primary metric. Therefore it is code-free to perform and everything can be automated. In addition, one great benifit AutoML provided is the Model expanation, which can be leveraged to obtain insights for feature importance.
- SDK Hyperdive hyperparameter tuning do need details sampling method, policy, primary metric name, primary metric goal, and hyperparameters. However, SDK Hyperdive hyperparameter tuning allows the user to control more details regarding the training algorithms and other details. -----------------------------------------------------------------------------------------------------------------------------------------------------------

## Future work

**What are some areas of improvement for future experiments? Why might these improvements help the model?**
- More feature engineering to get suitable features for training. 
- Try more regression algorithms.
- Add more hyperparameters for tuning such as batch size, etc.
-----------------------------------------------------------------------------------------------------------------------------------------------------------

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

The compute cluster was deletely and the image of cluser marked for deletion was included in the project separately, as "cluster-deleting-screenshot.png".




